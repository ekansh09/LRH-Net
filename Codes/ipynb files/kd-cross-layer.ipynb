{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29ea79",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:08.732326Z",
     "iopub.status.busy": "2022-06-29T15:28:08.730794Z",
     "iopub.status.idle": "2022-06-29T15:28:11.580486Z",
     "shell.execute_reply": "2022-06-29T15:28:11.579478Z",
     "shell.execute_reply.started": "2022-05-30T09:37:20.799728Z"
    },
    "papermill": {
     "duration": 2.905919,
     "end_time": "2022-06-29T15:28:11.580642",
     "exception": false,
     "start_time": "2022-06-29T15:28:08.674723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.signal import resample\n",
    "import sklearn.metrics as skm\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import average_precision_score,precision_recall_curve,roc_curve\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from scipy.optimize import differential_evolution\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98d1f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:11.687571Z",
     "iopub.status.busy": "2022-06-29T15:28:11.686752Z",
     "iopub.status.idle": "2022-06-29T15:28:11.689370Z",
     "shell.execute_reply": "2022-06-29T15:28:11.688881Z",
     "shell.execute_reply.started": "2022-05-30T09:37:24.122005Z"
    },
    "papermill": {
     "duration": 0.059725,
     "end_time": "2022-06-29T15:28:11.689496",
     "exception": false,
     "start_time": "2022-06-29T15:28:11.629771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Resample(input_signal, src_fs, tar_fs):\n",
    "    '''\n",
    "    :param input_signal:输入信号\n",
    "    :param src_fs:输入信号采样率\n",
    "    :param tar_fs:输出信号采样率\n",
    "    :return:输出信号\n",
    "    '''\n",
    "    if src_fs != tar_fs:\n",
    "        dtype = input_signal.dtype\n",
    "        audio_len = input_signal.shape[1]\n",
    "        audio_time_max = 1.0 * (audio_len) / src_fs\n",
    "        src_time = 1.0 * np.linspace(0, audio_len, audio_len) / src_fs\n",
    "        tar_time = 1.0 * np.linspace(0, np.int(audio_time_max * tar_fs), np.int(audio_time_max * tar_fs)) / tar_fs\n",
    "        for i in range(input_signal.shape[0]):\n",
    "            if i == 0:\n",
    "                output_signal = np.interp(tar_time, src_time, input_signal[i, :]).astype(dtype)\n",
    "                output_signal = output_signal.reshape(1, len(output_signal))\n",
    "            else:\n",
    "                tmp = np.interp(tar_time, src_time, input_signal[i, :]).astype(dtype)\n",
    "                tmp = tmp.reshape(1, len(tmp))\n",
    "                output_signal = np.vstack((output_signal, tmp))\n",
    "    else:\n",
    "        output_signal = input_signal\n",
    "    return output_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0a7b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:11.790797Z",
     "iopub.status.busy": "2022-06-29T15:28:11.790179Z",
     "iopub.status.idle": "2022-06-29T15:28:11.792762Z",
     "shell.execute_reply": "2022-06-29T15:28:11.793148Z",
     "shell.execute_reply.started": "2022-05-30T09:37:24.137896Z"
    },
    "papermill": {
     "duration": 0.056813,
     "end_time": "2022-06-29T15:28:11.793291",
     "exception": false,
     "start_time": "2022-06-29T15:28:11.736478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(case, src_fs, tar_fs=257):\n",
    "    #case = case.replace('..', '.')\n",
    "    x = loadmat(case)\n",
    "    data = np.asarray(x['val'], dtype=np.float64)\n",
    "    data = Resample(data, src_fs, tar_fs)\n",
    "    return data\n",
    "\n",
    "def prepare_data(age, gender): \n",
    "    data = np.zeros(5,) #age, age_mask,female,male ,gender_mask \n",
    "    if age >= 0:\n",
    "        data[0] = age / 100\n",
    "        data[1] = 1\n",
    "    if 'F' in gender:\n",
    "        data[2] = 1\n",
    "        data[4] = 1\n",
    "    elif gender == 'Unknown':\n",
    "        data[4] = 0\n",
    "    elif 'f' in gender:\n",
    "        data[2] = 1\n",
    "        data[4] = 1\n",
    "    else:\n",
    "        data[3] = 1\n",
    "        data[4] = 1\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676c556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:11.904068Z",
     "iopub.status.busy": "2022-06-29T15:28:11.903333Z",
     "iopub.status.idle": "2022-06-29T15:28:11.905269Z",
     "shell.execute_reply": "2022-06-29T15:28:11.905744Z",
     "shell.execute_reply.started": "2022-05-30T09:37:24.152924Z"
    },
    "papermill": {
     "duration": 0.065549,
     "end_time": "2022-06-29T15:28:11.905863",
     "exception": false,
     "start_time": "2022-06-29T15:28:11.840314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "\n",
    "    def __init__(self, anno_pd, test=False, transform=None,class_codes=None, class_names=None, data_dir=None, selected_leads =None, leads =None, loader=load_data):\n",
    "        self.test = test\n",
    "        if self.test:\n",
    "            self.data = anno_pd['filename'].tolist()\n",
    "            self.fs = anno_pd['fs'].tolist()\n",
    "        else:\n",
    "            self.data = anno_pd['filename'].tolist()\n",
    "            self.class_codes = class_codes\n",
    "            self.class_names = class_names\n",
    "            self.selected_leads =selected_leads\n",
    "            self.leads = leads\n",
    "            self.classwise_sample_count = [int(anno_pd.iloc[:, i].values.sum()) for i in range(4,anno_pd.shape[1])]\n",
    "            self.labels = anno_pd.iloc[:, 4:].values\n",
    "            self.multi_labels = [self.labels[i, :] for i in range(self.labels.shape[0])]\n",
    "            self.age = anno_pd['age'].tolist()\n",
    "            self.gender = anno_pd['gender'].tolist()\n",
    "            self.fs = anno_pd['fs'].tolist()\n",
    "\n",
    "            self.fs = anno_pd['fs'].tolist()\n",
    "\n",
    "        self.transforms = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.loader = loader\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.test:\n",
    "            img_path = self.data[item]\n",
    "            fs = self.fs[item]\n",
    "            img = self.loader(self.data_dir + img_path, src_fs=fs)\n",
    "            \n",
    "            img = self.transforms(img)\n",
    "            return img, img_path\n",
    "        else:\n",
    "            img_name = self.data[item]\n",
    "            #print(img_name)\n",
    "            fs = self.fs[item]\n",
    "            age = self.age[item]\n",
    "            gender = self.gender[item]\n",
    "            age_gender = prepare_data(age, gender)\n",
    "            img = self.loader(img_name, src_fs=fs)\n",
    "            if self.selected_leads != None and  self.leads != None:\n",
    "                lead_pos = [self.leads[i] for i in self.selected_leads]\n",
    "                img = img[lead_pos,:]\n",
    "            label = self.multi_labels[item]\n",
    "        \n",
    "            \"\"\"\n",
    "            for i in range(img.shape[1]):\n",
    "                img[:, i] = ecg_preprocessing(img[:, i], wfun='db6', levels=9, type=2)\n",
    "            \"\"\"\n",
    "            img = self.transforms(img)\n",
    "            return img, torch.from_numpy(label).float(),torch.from_numpy(age_gender).float(),item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc5b7b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:12.018439Z",
     "iopub.status.busy": "2022-06-29T15:28:12.016852Z",
     "iopub.status.idle": "2022-06-29T15:28:12.019051Z",
     "shell.execute_reply": "2022-06-29T15:28:12.019484Z",
     "shell.execute_reply.started": "2022-05-30T09:37:24.17588Z"
    },
    "papermill": {
     "duration": 0.067192,
     "end_time": "2022-06-29T15:28:12.019610",
     "exception": false,
     "start_time": "2022-06-29T15:28:11.952418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, seq):\n",
    "        for t in self.transforms:\n",
    "            seq = t(seq)\n",
    "        return seq\n",
    "\n",
    "class ValClip(object):\n",
    "    def __init__(self, len=72000):\n",
    "        self.len = len\n",
    "\n",
    "    def __call__(self, seq):\n",
    "        if seq.shape[1] >= self.len:\n",
    "            seq = seq\n",
    "        else:\n",
    "            zeros_padding = np.zeros(shape=(seq.shape[0], self.len - seq.shape[1]), dtype=np.float32)\n",
    "            seq = np.hstack((seq, zeros_padding))\n",
    "        return seq\n",
    "class RandomClip(object):\n",
    "    def __init__(self, len=72000):\n",
    "        self.len = len\n",
    "\n",
    "    def __call__(self, seq):\n",
    "        if seq.shape[1] >= self.len:\n",
    "            start = random.randint(0, seq.shape[1] - self.len)\n",
    "            seq = seq[:, start:start+self.len]\n",
    "        else:\n",
    "            left = random.randint(0, self.len - seq.shape[1])\n",
    "            right = self.len - seq.shape[1] - left\n",
    "            zeros_padding1 = np.zeros(shape=(seq.shape[0], left), dtype=np.float32)\n",
    "            zeros_padding2 = np.zeros(shape=(seq.shape[0], right), dtype=np.float32)\n",
    "            seq = np.hstack((zeros_padding1, seq, zeros_padding2))\n",
    "        return seq\n",
    "class Normalize(object):\n",
    "    def __init__(self, type=\"0-1\"):\n",
    "        self.type = type\n",
    "\n",
    "    def __call__(self, seq):\n",
    "        if self.type == \"0-1\":\n",
    "            for i in range(seq.shape[0]):\n",
    "                if np.sum(seq[i, :]) == 0:\n",
    "                    seq[i, :] = seq[i, :]\n",
    "                else:\n",
    "                    seq[i, :] = (seq[i, :]-seq[i, :].min())/(seq[i, :].max()-seq[i, :].min())\n",
    "        elif self.type == \"mean-std\":\n",
    "            for i in range(seq.shape[0]):\n",
    "                if np.sum(seq[i, :]) == 0:\n",
    "                    seq[i, :] = seq[i, :]\n",
    "                else:\n",
    "                    seq[i, :] = (seq[i, :]-seq[i, :].mean())/seq[i, :].std()\n",
    "        elif self.type == \"none\":\n",
    "            seq = seq\n",
    "        else:\n",
    "            raise NameError('This normalization is not included!')\n",
    "        return seq\n",
    "    \n",
    "    \n",
    "class Retype(object):\n",
    "    def __call__(self, seq):\n",
    "        return seq.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0c3aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:12.120881Z",
     "iopub.status.busy": "2022-06-29T15:28:12.119373Z",
     "iopub.status.idle": "2022-06-29T15:28:12.121496Z",
     "shell.execute_reply": "2022-06-29T15:28:12.121880Z",
     "shell.execute_reply.started": "2022-05-30T09:37:24.202162Z"
    },
    "papermill": {
     "duration": 0.055579,
     "end_time": "2022-06-29T15:28:12.122001",
     "exception": false,
     "start_time": "2022-06-29T15:28:12.066422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "normlizetype = 'none'\n",
    "start = 0\n",
    "seq_length = 4096\n",
    "sample_ratio = 0.5\n",
    "data_transforms = {\n",
    "    'train': Compose([\n",
    "        #Reshape(),\n",
    "        #DownSample(sample_ratio),\n",
    "        #ZerosPadding(len=seq_length),\n",
    "        # ConstantStart(start=start, num=seq_length),\n",
    "        RandomClip(len=seq_length),\n",
    "        Normalize(normlizetype),\n",
    "        # RandomAddGaussian(),\n",
    "        # RandomScale(0.1),\n",
    "        # RandomAmplify(),\n",
    "        # Randomverflip(),\n",
    "        # Randomshift(),\n",
    "        # RandomStretch(0.02),\n",
    "        # RandomCrop(),\n",
    "        Retype()\n",
    "    ]),\n",
    "    'val': Compose([\n",
    "        #Reshape(),\n",
    "        #DownSample(sample_ratio),\n",
    "        #RandomClip(len=seq_length),\n",
    "        RandomClip(len=seq_length),\n",
    "        #ValClip(len=seq_length),           **********(CHECK)\n",
    "        #ZerosPadding(len=seq_length),\n",
    "        # ConstantStart(start=start, num=seq_length),\n",
    "        Normalize(normlizetype),\n",
    "        Retype()\n",
    "    ]),\n",
    "    'test': Compose([\n",
    "        #Reshape(),\n",
    "        #DownSample(sig_resample_len),\n",
    "        RandomClip(len=seq_length),\n",
    "        #ValClip(len=seq_length),\n",
    "        Normalize(normlizetype),\n",
    "        Retype()\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313d0c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:12.223482Z",
     "iopub.status.busy": "2022-06-29T15:28:12.222681Z",
     "iopub.status.idle": "2022-06-29T15:28:12.230162Z",
     "shell.execute_reply": "2022-06-29T15:28:12.229729Z",
     "shell.execute_reply.started": "2022-05-30T09:37:24.216458Z"
    },
    "papermill": {
     "duration": 0.06158,
     "end_time": "2022-06-29T15:28:12.230273",
     "exception": false,
     "start_time": "2022-06-29T15:28:12.168693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_clean_sub(path):\n",
    "    label_test = pd.read_csv(path, sep=\"\\t\", names=list(range(0, 3)))\n",
    "    label_test.columns = ['id', 'age', 'gender']  # 设置列名\n",
    "    return label_test\n",
    "\n",
    "class ECG(object):\n",
    "    \n",
    "\n",
    "\n",
    "    def __init__(self, data_dir, split='0',selected_leads = None, leads = None):\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.num_classes = 24\n",
    "        self.selected_leads = selected_leads\n",
    "        self.leads = leads\n",
    "        self.inputchannel = 12\n",
    "\n",
    "\n",
    "    def data_preprare(self, test=False):\n",
    "        if self.selected_leads == None:\n",
    "            print('Dataset Formation: All the 12 leads are considered')\n",
    "        else:\n",
    "            print('Dataset Formation: The selected leads are:',self.selected_leads)\n",
    "        if test:\n",
    "            train_path = self.data_dir+'train_split' + self.split + '.csv'\n",
    "            val_path = self.data_dir+'test_split' + self.split + '.csv'\n",
    "            train_pd = pd.read_csv(train_path)\n",
    "            val_pd = pd.read_csv(val_path)\n",
    "\n",
    "            train_dataset = dataset(anno_pd=train_pd, transform=data_transforms['train'], data_dir=self.data_dir)\n",
    "            val_dataset = dataset(anno_pd=val_pd, transform=data_transforms['val'], data_dir=self.data_dir)\n",
    "            return train_dataset, val_dataset\n",
    "        else:\n",
    "            train_path = self.data_dir+'train_split' + self.split + '.csv'\n",
    "            val_path = self.data_dir+'test_split' + self.split + '.csv'\n",
    "            train_pd = pd.read_csv(train_path)\n",
    "            val_pd = pd.read_csv(val_path)\n",
    "            codes_mapping = pd.read_csv('../input/physionet-2020/dx_mapping_scored.csv')\n",
    "            class_codes = train_pd.columns.values[4:]\n",
    "            class_names = [codes_mapping[codes_mapping['SNOMED CT Code']==int(label)]['Dx'].values[0] for label in class_codes]\n",
    "            \n",
    "            \n",
    "            \n",
    "            train_dataset = dataset(anno_pd=train_pd, transform=data_transforms['train'],class_codes=class_codes,class_names=class_names, data_dir=self.data_dir, selected_leads = self.selected_leads, leads = self.leads)\n",
    "            val_dataset = dataset(anno_pd=val_pd, transform=data_transforms['val'],class_codes=class_codes,class_names = class_names, data_dir=self.data_dir,selected_leads=self.selected_leads,leads = self.leads)\n",
    "            return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e97b69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:12.329100Z",
     "iopub.status.busy": "2022-06-29T15:28:12.328375Z",
     "iopub.status.idle": "2022-06-29T15:28:22.853469Z",
     "shell.execute_reply": "2022-06-29T15:28:22.852947Z",
     "shell.execute_reply.started": "2022-05-30T09:37:24.237641Z"
    },
    "papermill": {
     "duration": 10.576547,
     "end_time": "2022-06-29T15:28:22.853597",
     "exception": false,
     "start_time": "2022-06-29T15:28:12.277050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "#import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12041a50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:22.967453Z",
     "iopub.status.busy": "2022-06-29T15:28:22.966744Z",
     "iopub.status.idle": "2022-06-29T15:28:22.968818Z",
     "shell.execute_reply": "2022-06-29T15:28:22.969222Z",
     "shell.execute_reply.started": "2022-05-30T09:37:37.546084Z"
    },
    "papermill": {
     "duration": 0.06114,
     "end_time": "2022-06-29T15:28:22.969357",
     "exception": false,
     "start_time": "2022-06-29T15:28:22.908217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "leads = {'I':0,'II':1,'III':2,'AVR':3,'AVL':4,'AVF':5,'V1':6,'V2':7,'V3':8,'V4':9,'V5':10,'V6':11}\n",
    "six_leads = ['I','II','III','AVR','AVL','AVF']\n",
    "four_leads = ['I','II','III','V2']\n",
    "three_leads = ['I','II','V2']\n",
    "two_leads = ['I','II']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdcb7f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:23.079869Z",
     "iopub.status.busy": "2022-06-29T15:28:23.079356Z",
     "iopub.status.idle": "2022-06-29T15:28:23.439674Z",
     "shell.execute_reply": "2022-06-29T15:28:23.440183Z",
     "shell.execute_reply.started": "2022-05-30T09:37:37.560341Z"
    },
    "papermill": {
     "duration": 0.418528,
     "end_time": "2022-06-29T15:28:23.440366",
     "exception": false,
     "start_time": "2022-06-29T15:28:23.021838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "batch_size = 64\n",
    "datasets['train'], datasets['val'] = ECG(\"../input/physionet20205folds/\", \"3\",six_leads,leads).data_preprare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0accd93c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:23.636972Z",
     "iopub.status.busy": "2022-06-29T15:28:23.631659Z",
     "iopub.status.idle": "2022-06-29T15:28:23.639671Z",
     "shell.execute_reply": "2022-06-29T15:28:23.640758Z",
     "shell.execute_reply.started": "2022-05-30T09:37:37.876729Z"
    },
    "papermill": {
     "duration": 0.097198,
     "end_time": "2022-06-29T15:28:23.640958",
     "exception": false,
     "start_time": "2022-06-29T15:28:23.543760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(datasets['train'],batch_size= batch_size,shuffle = True)\n",
    "valid_dataloader = DataLoader(datasets['val'],batch_size=batch_size,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b8dc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:23.825687Z",
     "iopub.status.busy": "2022-06-29T15:28:23.824521Z",
     "iopub.status.idle": "2022-06-29T15:28:24.418066Z",
     "shell.execute_reply": "2022-06-29T15:28:24.417093Z",
     "shell.execute_reply.started": "2022-05-30T09:37:37.88513Z"
    },
    "papermill": {
     "duration": 0.694598,
     "end_time": "2022-06-29T15:28:24.418232",
     "exception": false,
     "start_time": "2022-06-29T15:28:23.723634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "signals, class_labels,age_gender,indices = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc3d18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:24.543222Z",
     "iopub.status.busy": "2022-06-29T15:28:24.542424Z",
     "iopub.status.idle": "2022-06-29T15:28:24.546429Z",
     "shell.execute_reply": "2022-06-29T15:28:24.545847Z",
     "shell.execute_reply.started": "2022-05-30T09:37:38.743377Z"
    },
    "papermill": {
     "duration": 0.068008,
     "end_time": "2022-06-29T15:28:24.546563",
     "exception": false,
     "start_time": "2022-06-29T15:28:24.478555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(signals.shape,age_gender.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac1432",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:24.676220Z",
     "iopub.status.busy": "2022-06-29T15:28:24.675389Z",
     "iopub.status.idle": "2022-06-29T15:28:32.779584Z",
     "shell.execute_reply": "2022-06-29T15:28:32.780380Z",
     "shell.execute_reply.started": "2022-05-30T09:37:38.75265Z"
    },
    "papermill": {
     "duration": 8.172385,
     "end_time": "2022-06-29T15:28:32.780559",
     "exception": false,
     "start_time": "2022-06-29T15:28:24.608174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, cohen_kappa_score\n",
    "!pip install torchsummary \n",
    "from torchsummary import summary\n",
    "from functools import partial\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437774f",
   "metadata": {
    "papermill": {
     "duration": 0.058615,
     "end_time": "2022-06-29T15:28:32.894990",
     "exception": false,
     "start_time": "2022-06-29T15:28:32.836375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **SEResnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c0240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:33.024040Z",
     "iopub.status.busy": "2022-06-29T15:28:33.023273Z",
     "iopub.status.idle": "2022-06-29T15:28:33.025234Z",
     "shell.execute_reply": "2022-06-29T15:28:33.025607Z",
     "shell.execute_reply.started": "2022-05-30T09:37:49.436429Z"
    },
    "papermill": {
     "duration": 0.067081,
     "end_time": "2022-06-29T15:28:33.025739",
     "exception": false,
     "start_time": "2022-06-29T15:28:32.958658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd337f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:33.141672Z",
     "iopub.status.busy": "2022-06-29T15:28:33.140737Z",
     "iopub.status.idle": "2022-06-29T15:28:33.142782Z",
     "shell.execute_reply": "2022-06-29T15:28:33.143171Z",
     "shell.execute_reply.started": "2022-05-30T09:37:49.448738Z"
    },
    "papermill": {
     "duration": 0.062137,
     "end_time": "2022-06-29T15:28:33.143304",
     "exception": false,
     "start_time": "2022-06-29T15:28:33.081167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv3x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=7, stride=stride,\n",
    "                     padding=3, bias=False)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da9962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:33.263339Z",
     "iopub.status.busy": "2022-06-29T15:28:33.262610Z",
     "iopub.status.idle": "2022-06-29T15:28:33.264493Z",
     "shell.execute_reply": "2022-06-29T15:28:33.264852Z",
     "shell.execute_reply.started": "2022-05-30T09:37:49.461964Z"
    },
    "papermill": {
     "duration": 0.066653,
     "end_time": "2022-06-29T15:28:33.264971",
     "exception": false,
     "start_time": "2022-06-29T15:28:33.198318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None,is_last = False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = conv3x1(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x1(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.se = SELayer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        preact = out\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584803b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:33.530019Z",
     "iopub.status.busy": "2022-06-29T15:28:33.525147Z",
     "iopub.status.idle": "2022-06-29T15:28:33.532812Z",
     "shell.execute_reply": "2022-06-29T15:28:33.531990Z",
     "shell.execute_reply.started": "2022-05-30T09:37:49.479561Z"
    },
    "papermill": {
     "duration": 0.212927,
     "end_time": "2022-06-29T15:28:33.532956",
     "exception": false,
     "start_time": "2022-06-29T15:28:33.320029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, in_channel=12, out_channel=24, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv1d(in_channel, 64, kernel_size=15, stride=2, padding=7,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(5, 10)\n",
    "        self.fc = nn.Linear(512 * block.expansion + 10, out_channel)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        \"\"\"\n",
    "                          \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        \n",
    "        layers.append(block(self.inplanes, planes, stride, downsample,is_last=(blocks == 1)))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            \n",
    "            layers.append(block(self.inplanes, planes,is_last=(i == blocks-1)))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, ag, is_feat=False, preact=False):\n",
    "        \n",
    "        #print(\"input:\",x.shape, ag.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        f0 = x\n",
    "        x = self.maxpool(x)\n",
    "        f1 = x\n",
    "\n",
    "        x, f2_pre = self.layer1(x)\n",
    "        f2 = x\n",
    "        x, f3_pre = self.layer2(x)\n",
    "        f3 = x\n",
    "        x, f4_pre = self.layer3(x)\n",
    "        f4 = x\n",
    "        x, f5_pre = self.layer4(x)\n",
    "        f5 = x\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        f6 = x\n",
    "        \n",
    "        ag = self.fc1(ag)\n",
    "        x = torch.cat((ag, x), dim=1)\n",
    "        #print(\"x:\",x.shape)\n",
    "        x = self.fc(x)\n",
    "        #x = self.sig(x)\n",
    "\n",
    "        if is_feat:\n",
    "            if preact:\n",
    "                return [f0, f1, f2_pre, f3_pre, f4_pre, f5_pre, f6], x\n",
    "            else:\n",
    "                return [f0, f1, f2, f3, f4,f5,f6], x\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a6abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:33.650308Z",
     "iopub.status.busy": "2022-06-29T15:28:33.649548Z",
     "iopub.status.idle": "2022-06-29T15:28:33.651481Z",
     "shell.execute_reply": "2022-06-29T15:28:33.651906Z",
     "shell.execute_reply.started": "2022-05-30T09:37:49.50938Z"
    },
    "papermill": {
     "duration": 0.06293,
     "end_time": "2022-06-29T15:28:33.652032",
     "exception": false,
     "start_time": "2022-06-29T15:28:33.589102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resnet18(pretrained=False,in_lead = 12, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2],in_channel = in_lead, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2b97b",
   "metadata": {
    "papermill": {
     "duration": 0.055261,
     "end_time": "2022-06-29T15:28:33.762433",
     "exception": false,
     "start_time": "2022-06-29T15:28:33.707172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Custom Resnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28633758",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:33.878894Z",
     "iopub.status.busy": "2022-06-29T15:28:33.878174Z",
     "iopub.status.idle": "2022-06-29T15:28:33.880357Z",
     "shell.execute_reply": "2022-06-29T15:28:33.880768Z",
     "shell.execute_reply.started": "2022-05-30T09:37:49.523729Z"
    },
    "papermill": {
     "duration": 0.063033,
     "end_time": "2022-06-29T15:28:33.880929",
     "exception": false,
     "start_time": "2022-06-29T15:28:33.817896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv1d(in_planes, out_planes, kernel_size, strides=1,padding='same', bias=True):\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=strides,padding=padding, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee87bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:34.002600Z",
     "iopub.status.busy": "2022-06-29T15:28:34.001589Z",
     "iopub.status.idle": "2022-06-29T15:28:34.003510Z",
     "shell.execute_reply": "2022-06-29T15:28:34.003934Z",
     "shell.execute_reply.started": "2022-05-30T09:37:49.534931Z"
    },
    "papermill": {
     "duration": 0.064669,
     "end_time": "2022-06-29T15:28:34.004076",
     "exception": false,
     "start_time": "2022-06-29T15:28:33.939407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Cust_SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=8):\n",
    "        super(Cust_SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607562f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:34.129400Z",
     "iopub.status.busy": "2022-06-29T15:28:34.128375Z",
     "iopub.status.idle": "2022-06-29T15:28:34.130256Z",
     "shell.execute_reply": "2022-06-29T15:28:34.130662Z",
     "shell.execute_reply.started": "2022-05-30T09:37:49.551985Z"
    },
    "papermill": {
     "duration": 0.069772,
     "end_time": "2022-06-29T15:28:34.130797",
     "exception": false,
     "start_time": "2022-06-29T15:28:34.061025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Cust_BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, out_channels, kernel_size=8, stride=1, padding='same', bias=False, downsample=None, is_last = False):\n",
    "        super().__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = conv1d(in_planes, out_channels,kernel_size, strides = (1 if not downsample else 2), padding= ('same' if not downsample else 3))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = conv1d(out_channels, out_channels,kernel_size, strides = 1)\n",
    "        self.se = Cust_SELayer(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.se(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "#         print(x.size())\n",
    "#         print(residual.size())\n",
    "#         print(out.size())\n",
    "        out += residual\n",
    "        preact = out\n",
    "        out = self.relu(out)\n",
    "        out = self.bn2(out)    \n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa83f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:34.259021Z",
     "iopub.status.busy": "2022-06-29T15:28:34.258231Z",
     "iopub.status.idle": "2022-06-29T15:28:34.259821Z",
     "shell.execute_reply": "2022-06-29T15:28:34.260638Z",
     "shell.execute_reply.started": "2022-05-30T09:37:49.567521Z"
    },
    "papermill": {
     "duration": 0.073709,
     "end_time": "2022-06-29T15:28:34.260771",
     "exception": false,
     "start_time": "2022-06-29T15:28:34.187062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Cust_Resnet(nn.Module):\n",
    "    def __init__(self, block, layers, input_channel, num_classes):\n",
    "        super(Cust_Resnet, self).__init__()\n",
    "        \n",
    "        self.in_channel = 16\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(input_channel, 16, kernel_size=8, padding = 'same')\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, layers[0],out_channels = 16 )\n",
    "        self.layer2 = self._make_layer(block,layers[1],out_channels = 32)\n",
    "        self.layer3 = self._make_layer(block,layers[2],out_channels = 64)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(5, 10)\n",
    "        self.fc1 = nn.Linear(64+10, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def _make_layer(self, block, num_residual_block, out_channels):\n",
    "        downsample = None\n",
    "        layers = []\n",
    "        if self.in_channel != out_channels * block.expansion:\n",
    "                downsample = conv1d(self.in_channel, out_channels*block.expansion,kernel_size=1, strides=2, padding=0)\n",
    "\n",
    "        layers.append( block(in_planes=self.in_channel,out_channels =out_channels,stride=1,downsample=downsample, is_last=(num_residual_block==1)))\n",
    "        self.in_channel = out_channels * block.expansion\n",
    "        for i in range(1, num_residual_block):\n",
    "            layers.append(block(self.in_channel, out_channels,is_last=(i==num_residual_block-1)))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x,ag,is_feat=False, preact=False):\n",
    "        x= self.conv1(x)\n",
    "        x= self.relu(x)\n",
    "        f0 = x\n",
    "        \n",
    "        x,f1_pre= self.layer1(x)\n",
    "        f1 = x\n",
    "        \n",
    "        x,f2_pre= self.layer2(x)\n",
    "        f2 = x\n",
    "        \n",
    "        x,f3_pre= self.layer3(x)\n",
    "        f3 = x\n",
    "        \n",
    "        x= self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        f4 = x\n",
    "        \n",
    "        ag = self.fc(ag)\n",
    "        x = torch.cat((ag, x), dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "#         x = self.sigmoid(x)\n",
    "\n",
    "        if is_feat:\n",
    "            if preact:\n",
    "                return [f0, f1_pre, f2_pre, f3_pre, f4], x\n",
    "            else:\n",
    "                return [f0, f1, f2, f3, f4], x\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c6b35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:34.377061Z",
     "iopub.status.busy": "2022-06-29T15:28:34.376307Z",
     "iopub.status.idle": "2022-06-29T15:28:34.378737Z",
     "shell.execute_reply": "2022-06-29T15:28:34.378301Z",
     "shell.execute_reply.started": "2022-05-30T09:37:49.590724Z"
    },
    "papermill": {
     "duration": 0.060607,
     "end_time": "2022-06-29T15:28:34.378841",
     "exception": false,
     "start_time": "2022-06-29T15:28:34.318234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CustomResnet(input_channel, num_classes=24):\n",
    "    return Cust_Resnet(Cust_BasicBlock, [1,1,1], input_channel, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca6b8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:34.493469Z",
     "iopub.status.busy": "2022-06-29T15:28:34.492695Z",
     "iopub.status.idle": "2022-06-29T15:28:34.834393Z",
     "shell.execute_reply": "2022-06-29T15:28:34.834907Z",
     "shell.execute_reply.started": "2022-05-30T09:37:49.604739Z"
    },
    "papermill": {
     "duration": 0.400164,
     "end_time": "2022-06-29T15:28:34.835057",
     "exception": false,
     "start_time": "2022-06-29T15:28:34.434893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(16, 6, 5000, dtype=torch.float)\n",
    "age_dummy_input = torch.randn(16, 5, dtype=torch.float)\n",
    "\n",
    "dummy_model = CustomResnet(input_channel=6)\n",
    "ft,op = dummy_model(dummy_input,age_dummy_input,is_feat = True)\n",
    "print(len(ft),op.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb09ff",
   "metadata": {
    "papermill": {
     "duration": 0.057466,
     "end_time": "2022-06-29T15:28:34.950063",
     "exception": false,
     "start_time": "2022-06-29T15:28:34.892597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **SEMCKD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdceae5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:35.072177Z",
     "iopub.status.busy": "2022-06-29T15:28:35.071268Z",
     "iopub.status.idle": "2022-06-29T15:28:35.073185Z",
     "shell.execute_reply": "2022-06-29T15:28:35.073569Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.042578Z"
    },
    "papermill": {
     "duration": 0.06796,
     "end_time": "2022-06-29T15:28:35.073705",
     "exception": false,
     "start_time": "2022-06-29T15:28:35.005745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLPEmbed(nn.Module):\n",
    "    \"\"\"non-linear embed by MLP\"\"\"\n",
    "    def __init__(self, dim_in=1024, dim_out=128):\n",
    "        super(MLPEmbed, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim_in, 2 * dim_out)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear2 = nn.Linear(2 * dim_out, dim_out)\n",
    "        self.l2norm = Normalize(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.l2norm(self.linear2(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Normalize(nn.Module):\n",
    "    \"\"\"normalization layer\"\"\"\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e063e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:35.195214Z",
     "iopub.status.busy": "2022-06-29T15:28:35.194516Z",
     "iopub.status.idle": "2022-06-29T15:28:35.197006Z",
     "shell.execute_reply": "2022-06-29T15:28:35.196598Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.057166Z"
    },
    "papermill": {
     "duration": 0.066775,
     "end_time": "2022-06-29T15:28:35.197110",
     "exception": false,
     "start_time": "2022-06-29T15:28:35.130335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "           \n",
    "class AAEmbed(nn.Module):\n",
    "    \"\"\"non-linear embed by MLP\"\"\"\n",
    "    def __init__(self, num_input_channels=1024, num_target_channels=128):\n",
    "        super(AAEmbed, self).__init__()\n",
    "        self.num_mid_channel = 2 * num_target_channels\n",
    "        \n",
    "        def conv1x1(in_channels, out_channels, stride=1):\n",
    "            return nn.Conv1d(in_channels, out_channels, kernel_size=1, padding=0, stride=stride, bias=False)\n",
    "        def conv3x3(in_channels, out_channels, stride=1):\n",
    "            return nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        \n",
    "        self.conv1d = conv1x1(num_input_channels, self.num_mid_channel)\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            conv1x1(num_input_channels, self.num_mid_channel),\n",
    "            nn.BatchNorm1d(self.num_mid_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv3x3(self.num_mid_channel, self.num_mid_channel),\n",
    "            nn.BatchNorm1d(self.num_mid_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv1x1(self.num_mid_channel, num_target_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5db85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:35.327428Z",
     "iopub.status.busy": "2022-06-29T15:28:35.326675Z",
     "iopub.status.idle": "2022-06-29T15:28:35.329229Z",
     "shell.execute_reply": "2022-06-29T15:28:35.328738Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.078645Z"
    },
    "papermill": {
     "duration": 0.076723,
     "end_time": "2022-06-29T15:28:35.329356",
     "exception": false,
     "start_time": "2022-06-29T15:28:35.252633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfA(nn.Module):\n",
    "    \"\"\"Cross layer Self Attention\"\"\"\n",
    "    def __init__(self, s_len, t_len, input_channel, s_n, s_t, factor=4): \n",
    "        super(SelfA, self).__init__()\n",
    "          \n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        for i in range(t_len):\n",
    "            setattr(self, 'key_weight'+str(i), MLPEmbed(input_channel, input_channel//factor))\n",
    "        for i in range(s_len):\n",
    "            setattr(self, 'query_weight'+str(i), MLPEmbed(input_channel, input_channel//factor))\n",
    "        \n",
    "        for i in range(s_len):\n",
    "            for j in range(t_len):\n",
    "                setattr(self, 'regressor'+str(i)+str(j), AAEmbed(s_n[i], s_t[j]))\n",
    "               \n",
    "    def forward(self, feat_s, feat_t):\n",
    "        \n",
    "        sim_t = list(range(len(feat_t)))\n",
    "        sim_s = list(range(len(feat_s)))\n",
    "        bsz = feat_s[0].shape[0]\n",
    "        #print('bsz:',bsz)\n",
    "        # similarity matrix\n",
    "        for i in range(len(feat_t)):\n",
    "            sim_temp = feat_t[i].reshape(bsz, -1) #(batch,size,n,m) -> (batch_size,n*m)\n",
    "            sim_t[i] = torch.matmul(sim_temp, sim_temp.t()) #(batch_sizexn) X (batch_sizexn) Reshapes all the features to batch_sizeXbatch_size\n",
    "            \n",
    "        for i in range(len(feat_s)):\n",
    "            sim_temp = feat_s[i].reshape(bsz, -1)  #(batch,size,n,m) -> (batch_size,n*m)\n",
    "            sim_s[i] = torch.matmul(sim_temp, sim_temp.t()) #(batch_sizexn) X (batch_sizexn) Reshapes all the features to batch_sizeXbatch_size\n",
    "\n",
    "        \n",
    "        # key of target layers    \n",
    "        proj_key = self.key_weight0(sim_t[0])\n",
    "        \n",
    "        proj_key = proj_key[:, :, None]\n",
    "        \n",
    "        for i in range(1, len(sim_t)):\n",
    "            temp_proj_key = getattr(self, 'key_weight'+str(i))(sim_t[i])\n",
    "            proj_key =  torch.cat([proj_key, temp_proj_key[:, :, None]], 2)\n",
    "        #print('proj_key:',proj_key.shape)\n",
    "        \n",
    "        # query of source layers   \n",
    "        proj_query = self.query_weight0(sim_s[0])\n",
    "        proj_query = proj_query[:, None, :]\n",
    "        for i in range(1, len(sim_s)):\n",
    "            temp_proj_query = getattr(self, 'query_weight'+str(i))(sim_s[i])\n",
    "            proj_query = torch.cat([proj_query, temp_proj_query[:, None, :]], 1)\n",
    "        #print('proj_query:',proj_query.shape)\n",
    "        \n",
    "        # attention weight\n",
    "        energy = torch.bmm(proj_query, proj_key) # batch_size X No.stu feature X No.tea feature\n",
    "        #print('bmm:',energy.shape)\n",
    "        attention = F.softmax(energy, dim = -1)\n",
    "        #print('attn:',attention.shape)\n",
    "        \n",
    "        # feature space alignment\n",
    "        proj_value_stu = []\n",
    "        value_tea = []\n",
    "        for i in range(len(sim_s)):\n",
    "            proj_value_stu.append([])\n",
    "            value_tea.append([])\n",
    "            \n",
    "            for j in range(len(sim_t)):\n",
    "                s_H, t_H = feat_s[i].shape[2], feat_t[j].shape[2]\n",
    "                \n",
    "                if s_H > t_H:\n",
    "                    inputt = F.adaptive_avg_pool1d(feat_s[i], (t_H))\n",
    "                    proj_value_stu[i].append(getattr(self, 'regressor'+str(i)+str(j))(inputt))\n",
    "                    value_tea[i].append(feat_t[j])\n",
    "                elif s_H < t_H or s_H == t_H:\n",
    "                    target = F.adaptive_avg_pool1d(feat_t[j], s_H)\n",
    "                    proj_value_stu[i].append(getattr(self, 'regressor'+str(i)+str(j))(feat_s[i]))\n",
    "                    value_tea[i].append(target)\n",
    "        #print(len(proj_value_stu),len(value_tea))\n",
    "        return proj_value_stu, value_tea, attention\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7d207",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:35.471018Z",
     "iopub.status.busy": "2022-06-29T15:28:35.449509Z",
     "iopub.status.idle": "2022-06-29T15:28:35.473227Z",
     "shell.execute_reply": "2022-06-29T15:28:35.472770Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.105761Z"
    },
    "papermill": {
     "duration": 0.088087,
     "end_time": "2022-06-29T15:28:35.473351",
     "exception": false,
     "start_time": "2022-06-29T15:28:35.385264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if the input is a number.\n",
    "def is_number(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Load a table with row and column names.\n",
    "def load_table(table_file):\n",
    "    # The table should have the following form:\n",
    "    #\n",
    "    # ,    a,   b,   c\n",
    "    # a, 1.2, 2.3, 3.4\n",
    "    # b, 4.5, 5.6, 6.7\n",
    "    # c, 7.8, 8.9, 9.0\n",
    "    #\n",
    "    table = list()\n",
    "    with open(table_file, 'r') as f:\n",
    "        for i, l in enumerate(f):\n",
    "            arrs = [arr.strip() for arr in l.split(',')]\n",
    "            table.append(arrs)\n",
    "\n",
    "    # Define the numbers of rows and columns and check for errors.\n",
    "    num_rows = len(table)-1\n",
    "    if num_rows<1:\n",
    "        raise Exception('The table {} is empty.'.format(table_file))\n",
    "\n",
    "    num_cols = set(len(table[i])-1 for i in range(num_rows))\n",
    "    if len(num_cols)!=1:\n",
    "        raise Exception('The table {} has rows with different lengths.'.format(table_file))\n",
    "    num_cols = min(num_cols)\n",
    "    if num_cols<1:\n",
    "        raise Exception('The table {} is empty.'.format(table_file))\n",
    "\n",
    "    # Find the row and column labels.\n",
    "    rows = [table[0][j+1] for j in range(num_rows)]\n",
    "    cols = [table[i+1][0] for i in range(num_cols)]\n",
    "\n",
    "    # Find the entries of the table.\n",
    "    values = np.zeros((num_rows, num_cols))\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            value = table[i+1][j+1]\n",
    "            if is_number(value):\n",
    "                values[i, j] = float(value)\n",
    "            else:\n",
    "                values[i, j] = float('nan')\n",
    "\n",
    "    return rows, cols, values\n",
    "\n",
    "\n",
    "\n",
    "# Load weights.\n",
    "def load_weights(weight_file, classes):\n",
    "    # Load the weight matrix.\n",
    "    rows, cols, values = load_table(weight_file)\n",
    "    assert(rows == cols)\n",
    "    num_rows = len(rows)\n",
    "\n",
    "    # Assign the entries of the weight matrix with rows and columns corresponding to the classes.\n",
    "    num_classes = len(classes)\n",
    "    weights = np.zeros((num_classes, num_classes), dtype=np.float64)\n",
    "    for i, a in enumerate(rows):\n",
    "        if a in classes:\n",
    "            k = classes.index(a)\n",
    "            for j, b in enumerate(rows):\n",
    "                if b in classes:\n",
    "                    l = classes.index(b)\n",
    "                    weights[k, l] = values[i, j]\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Compute modified confusion matrix for multi-class, multi-label tasks.\n",
    "def compute_modified_confusion_matrix(labels, outputs):\n",
    "    # Compute a binary multi-class, multi-label confusion matrix, where the rows\n",
    "    # are the labels and the columns are the outputs.\n",
    "    num_recordings, num_classes = np.shape(labels)\n",
    "    A = np.zeros((num_classes, num_classes))\n",
    "\n",
    "    # Iterate over all of the recordings.\n",
    "    for i in range(num_recordings):\n",
    "        # Calculate the number of positive labels and/or outputs.\n",
    "        normalization = float(max(np.sum(np.any((labels[i, :], outputs[i, :]), axis=0)), 1))\n",
    "        # Iterate over all of the classes.\n",
    "        for j in range(num_classes):\n",
    "            # Assign full and/or partial credit for each positive class.\n",
    "            if labels[i, j]:\n",
    "                for k in range(num_classes):\n",
    "                    if outputs[i, k]:\n",
    "                        A[j, k] += 1.0/normalization\n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "# Compute the evaluation metric for the Challenge.\n",
    "def compute_challenge_metric(weights, labels, outputs, classes, normal_class):\n",
    "    num_recordings, num_classes = np.shape(labels)\n",
    "    normal_index = classes.index(normal_class)\n",
    "\n",
    "    # Compute the observed score.\n",
    "    A = compute_modified_confusion_matrix(labels, outputs)\n",
    "    observed_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the correct label(s).\n",
    "    correct_outputs = labels\n",
    "    A = compute_modified_confusion_matrix(labels, correct_outputs)\n",
    "    correct_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the normal class.\n",
    "    inactive_outputs = np.zeros((num_recordings, num_classes), dtype=np.bool)\n",
    "    inactive_outputs[:, normal_index] = 1\n",
    "    A = compute_modified_confusion_matrix(labels, inactive_outputs)\n",
    "    inactive_score = np.nansum(weights * A)\n",
    "\n",
    "    if correct_score != inactive_score:\n",
    "        normalized_score = float(observed_score - inactive_score) / float(correct_score - inactive_score)\n",
    "    else:\n",
    "        normalized_score = float('nan')\n",
    "\n",
    "    return normalized_score\n",
    "\n",
    "#F1score\n",
    "def cal_Acc(y_true, y_pre, threshold=0.5, num_classes=9, beta=2, normal=False):\n",
    "    \n",
    "    y_true = y_true.cpu().detach().numpy().astype(np.int)\n",
    "\n",
    "    y_label = np.zeros(y_true.shape)\n",
    "    # Generate the one hot encoding labels\n",
    "    _, y_pre_label = torch.max(y_pre, 1)\n",
    "    y_pre_label = y_pre_label.cpu().detach().numpy()\n",
    "\n",
    "    y_label[np.arange(y_true.shape[0]), y_pre_label] = 1\n",
    "    y_prob = y_pre.cpu().detach().numpy()\n",
    "    y_pre = y_pre.cpu().detach().numpy() >= threshold\n",
    "\n",
    "\n",
    "    y_label = y_label + y_pre\n",
    "    y_label[y_label > 1.1] = 1\n",
    "\n",
    "    labels = y_true\n",
    "    binary_outputs = y_label\n",
    "\n",
    "\n",
    "    # Define the weights, the SNOMED CT code for the normal class, and equivalent SNOMED CT codes.\n",
    "    weights_file = '../input/physionet-2020/weights.csv'\n",
    "    normal_class = '426783006'\n",
    "\n",
    "    # Get the label\n",
    "    label_file_dir = '../input/physionet-2020/dx_mapping_scored.csv'\n",
    "    label_file = pd.read_csv(label_file_dir)\n",
    "    equivalent_classes = ['59118001', '63593006', '17338001']\n",
    "    classes = sorted(list(set([str(name) for name in label_file['SNOMED CT Code']]) - set(equivalent_classes)))\n",
    "    \n",
    "\n",
    "    weights = load_weights(weights_file, classes)\n",
    "\n",
    "    # Only consider classes that are scored with the Challenge metric.\n",
    "    indices = np.any(weights, axis=0) # Find indices of classes in weight matrix.\n",
    "    classes = [x for i, x in enumerate(classes) if indices[i]]\n",
    "    labels = labels[:, indices]\n",
    "    binary_outputs = binary_outputs[:, indices]\n",
    "    weights = weights[np.ix_(indices, indices)]\n",
    "\n",
    "    challenge_metric = compute_challenge_metric(weights, labels, binary_outputs, classes, normal_class)\n",
    "\n",
    "    # Return the results.\n",
    "    return challenge_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd2edd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:35.602963Z",
     "iopub.status.busy": "2022-06-29T15:28:35.602184Z",
     "iopub.status.idle": "2022-06-29T15:28:35.604624Z",
     "shell.execute_reply": "2022-06-29T15:28:35.604210Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.145Z"
    },
    "papermill": {
     "duration": 0.07564,
     "end_time": "2022-06-29T15:28:35.604737",
     "exception": false,
     "start_time": "2022-06-29T15:28:35.529097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute macro AUROC and macro AUPRC.\n",
    "def compute_auc(labels, outputs):\n",
    "    \n",
    "    num_recordings, num_classes = np.shape(labels)\n",
    "    outputs = outputs.cpu().detach().numpy().astype(np.int)\n",
    "    labels = labels.cpu().detach().numpy().astype(np.int)\n",
    "    # Compute and summarize the confusion matrices for each class across at distinct output values.\n",
    "    auroc = np.zeros(num_classes)\n",
    "    auprc = np.zeros(num_classes)\n",
    "\n",
    "    for k in range(num_classes):\n",
    "        # We only need to compute TPs, FPs, FNs, and TNs at distinct output values.\n",
    "        thresholds = np.unique(outputs[:, k])\n",
    "        thresholds = np.append(thresholds, thresholds[-1]+1)\n",
    "        thresholds = thresholds[::-1]\n",
    "        num_thresholds = len(thresholds)\n",
    "\n",
    "        # Initialize the TPs, FPs, FNs, and TNs.\n",
    "        tp = np.zeros(num_thresholds)\n",
    "        fp = np.zeros(num_thresholds)\n",
    "        fn = np.zeros(num_thresholds)\n",
    "        tn = np.zeros(num_thresholds)\n",
    "        fn[0] = np.sum(labels[:, k]==1)\n",
    "        tn[0] = np.sum(labels[:, k]==0)\n",
    "\n",
    "        # Find the indices that result in sorted output values.\n",
    "        idx = np.argsort(outputs[:, k])[::-1]\n",
    "\n",
    "        # Compute the TPs, FPs, FNs, and TNs for class k across thresholds.\n",
    "        i = 0\n",
    "        for j in range(1, num_thresholds):\n",
    "            # Initialize TPs, FPs, FNs, and TNs using values at previous threshold.\n",
    "            tp[j] = tp[j-1]\n",
    "            fp[j] = fp[j-1]\n",
    "            fn[j] = fn[j-1]\n",
    "            tn[j] = tn[j-1]\n",
    "\n",
    "            # Update the TPs, FPs, FNs, and TNs at i-th output value.\n",
    "            while i < num_recordings and outputs[idx[i], k] >= thresholds[j]:\n",
    "                if labels[idx[i], k]:\n",
    "                    tp[j] += 1\n",
    "                    fn[j] -= 1\n",
    "                else:\n",
    "                    fp[j] += 1\n",
    "                    tn[j] -= 1\n",
    "                i += 1\n",
    "\n",
    "        # Summarize the TPs, FPs, FNs, and TNs for class k.\n",
    "        tpr = np.zeros(num_thresholds)\n",
    "        tnr = np.zeros(num_thresholds)\n",
    "        ppv = np.zeros(num_thresholds)\n",
    "        npv = np.zeros(num_thresholds)\n",
    "\n",
    "        for j in range(num_thresholds):\n",
    "            if tp[j] + fn[j]:\n",
    "                tpr[j] = float(tp[j]) / float(tp[j] + fn[j])\n",
    "            else:\n",
    "                tpr[j] = float('nan')\n",
    "            if fp[j] + tn[j]:\n",
    "                tnr[j] = float(tn[j]) / float(fp[j] + tn[j])\n",
    "            else:\n",
    "                tnr[j] = float('nan')\n",
    "            if tp[j] + fp[j]:\n",
    "                ppv[j] = float(tp[j]) / float(tp[j] + fp[j])\n",
    "            else:\n",
    "                ppv[j] = float('nan')\n",
    "\n",
    "        #print(\"Sensitivity(tpr):\",tpr,\" Specificity:\",1-tnr)\n",
    "        # Compute AUROC as the area under a piecewise linear function with TPR/\n",
    "        # sensitivity (x-axis) and TNR/specificity (y-axis) and AUPRC as the area\n",
    "        # under a piecewise constant with TPR/recall (x-axis) and PPV/precision\n",
    "        # (y-axis) for class k.\n",
    "        for j in range(num_thresholds-1):\n",
    "            auroc[k] += 0.5 * (tpr[j+1] - tpr[j]) * (tnr[j+1] + tnr[j])\n",
    "            auprc[k] += (tpr[j+1] - tpr[j]) * ppv[j+1]\n",
    "\n",
    "    # Compute macro AUROC and macro AUPRC across classes.\n",
    "    #print(\"auroc shape:\",auroc.shape,\"\\nauroc:\",auroc) #Class-wise aucroc\n",
    "    macro_auroc = np.nanmean(auroc)\n",
    "    macro_auprc = np.nanmean(auprc)\n",
    "\n",
    "    return auroc, auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b921cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:35.723845Z",
     "iopub.status.busy": "2022-06-29T15:28:35.723080Z",
     "iopub.status.idle": "2022-06-29T15:28:35.724929Z",
     "shell.execute_reply": "2022-06-29T15:28:35.725452Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.173317Z"
    },
    "papermill": {
     "duration": 0.064189,
     "end_time": "2022-06-29T15:28:35.725578",
     "exception": false,
     "start_time": "2022-06-29T15:28:35.661389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class optim_genetics:\n",
    "    def __init__(self, target, outputs, classes):\n",
    "        self.target = target\n",
    "        self.outputs = outputs\n",
    "        weights_file = '../input/physionet-2020/weights.csv'\n",
    "        self.normal_class = '426783006'\n",
    "        equivalent_classes = [['713427006', '59118001'],\n",
    "                              ['284470004', '63593006'],\n",
    "                              ['427172004', '17338001']]\n",
    "\n",
    "        # Load the scored classes and the weights for the Challenge metric.\n",
    "        self.weights = load_weights(weights_file, classes)\n",
    "        self.classes = classes\n",
    "        # match classed ordering\n",
    "        # reorder = [self.classes.index(c) for c in classes]\n",
    "        # self.outputs = self.outputs[:, reorder]\n",
    "        # self.target = self.target[:, reorder]\n",
    "        stop = 1\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = copy.deepcopy(self.outputs)\n",
    "        outputs = outputs > x\n",
    "        outputs = np.array(outputs, dtype=int)\n",
    "        return -compute_challenge_metric(self.weights, self.target, outputs, self.classes, self.normal_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22561e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:35.846534Z",
     "iopub.status.busy": "2022-06-29T15:28:35.844426Z",
     "iopub.status.idle": "2022-06-29T15:28:35.849187Z",
     "shell.execute_reply": "2022-06-29T15:28:35.848685Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.190547Z"
    },
    "papermill": {
     "duration": 0.067433,
     "end_time": "2022-06-29T15:28:35.849323",
     "exception": false,
     "start_time": "2022-06-29T15:28:35.781890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_thresholds(t,y, class_codes):\n",
    "\n",
    "    N = 24\n",
    "    f1prcT = np.zeros((N,))\n",
    "    f1rocT = np.zeros((N,))\n",
    "\n",
    "    for j in range(N):\n",
    "        prc, rec, thr = precision_recall_curve(y_true=t[:, j], probas_pred=y[:, j])\n",
    "        fscore = 2 * prc * rec / (prc + rec)\n",
    "        idx = np.nanargmax(fscore)\n",
    "        f1prc = np.nanmax(fscore)\n",
    "        f1prcT[j] = thr[idx]\n",
    "\n",
    "        fpr, tpr, thr = roc_curve(y_true=t[:, j], y_score=y[:, j])\n",
    "        fscore = 2 * (1 - fpr) * tpr / (1 - fpr + tpr)\n",
    "        idx = np.nanargmax(fscore)\n",
    "        f1roc = np.nanmax(fscore)\n",
    "        f1rocT[j] = thr[idx]\n",
    "\n",
    "    population = np.random.rand(300, N)\n",
    "    for i in range(1, 99):\n",
    "        population[i, :] = i / 100\n",
    "\n",
    "#     print(f1prcT)\n",
    "#     print(f1rocT)\n",
    "    population[100] = f1rocT\n",
    "    population[101] = f1prcT\n",
    "    bounds = [(0, 1) for i in range(N)]\n",
    "    print('Differential_evolution started...')\n",
    "    result = differential_evolution(optim_genetics(t, y, class_codes), bounds=bounds, disp=True, init=population, workers=-1)\n",
    "    print('Differential_evolution ended...')\n",
    "#     print(result)\n",
    "    return result.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae571c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:36.031647Z",
     "iopub.status.busy": "2022-06-29T15:28:36.029501Z",
     "iopub.status.idle": "2022-06-29T15:28:36.034014Z",
     "shell.execute_reply": "2022-06-29T15:28:36.033581Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.207165Z"
    },
    "papermill": {
     "duration": 0.12855,
     "end_time": "2022-06-29T15:28:36.034149",
     "exception": false,
     "start_time": "2022-06-29T15:28:35.905599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_count = torch.cuda.device_count()\n",
    "    logging.info('using {} gpus'.format(device_count))\n",
    "    assert batch_size % device_count == 0, \"batch size should be divided by device count\"\n",
    "else:\n",
    "    warnings.warn(\"gpu is not available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_count = 1\n",
    "    logging.info('using {} cpu'.format(device_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9726540f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:36.150836Z",
     "iopub.status.busy": "2022-06-29T15:28:36.150238Z",
     "iopub.status.idle": "2022-06-29T15:28:36.152982Z",
     "shell.execute_reply": "2022-06-29T15:28:36.153396Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.287013Z"
    },
    "papermill": {
     "duration": 0.062713,
     "end_time": "2022-06-29T15:28:36.153534",
     "exception": false,
     "start_time": "2022-06-29T15:28:36.090821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(device_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac1c30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:36.273452Z",
     "iopub.status.busy": "2022-06-29T15:28:36.272571Z",
     "iopub.status.idle": "2022-06-29T15:28:36.274406Z",
     "shell.execute_reply": "2022-06-29T15:28:36.274830Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.298704Z"
    },
    "papermill": {
     "duration": 0.065075,
     "end_time": "2022-06-29T15:28:36.274961",
     "exception": false,
     "start_time": "2022-06-29T15:28:36.209886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the learning rate decay\n",
    "\n",
    "def get_optimizer(opt,model,lr,momentum,weight_decay):\n",
    "    \n",
    "    if opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,\n",
    "                                   momentum=momentum, weight_decay=weight_decay)\n",
    "    elif opt == 'adam':\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,\n",
    "                                    weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise Exception(\"optimizer not implement\")\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c23e29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:36.395297Z",
     "iopub.status.busy": "2022-06-29T15:28:36.394510Z",
     "iopub.status.idle": "2022-06-29T15:28:36.396466Z",
     "shell.execute_reply": "2022-06-29T15:28:36.396861Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.310805Z"
    },
    "papermill": {
     "duration": 0.066056,
     "end_time": "2022-06-29T15:28:36.396993",
     "exception": false,
     "start_time": "2022-06-29T15:28:36.330937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the learning rate decay\n",
    "def get_lr_scheduler(lr_scheduler,optimizer,steps,gamma):\n",
    "    \n",
    "    if lr_scheduler == 'step':\n",
    "        steps_list = [int(step) for step in steps.split(',')]\n",
    "        #print(steps_list)\n",
    "        lr_scheduler_fn = optim.lr_scheduler.MultiStepLR(optimizer, steps_list, gamma=gamma)\n",
    "    elif lr_scheduler == 'exp':\n",
    "        lr_scheduler_fn = optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
    "    elif lr_scheduler == 'stepLR':\n",
    "        steps = int(steps)\n",
    "        lr_scheduler_fn = optim.lr_scheduler.StepLR(optimizer, steps, gamma)\n",
    "    elif lr_scheduler == 'cos':\n",
    "        steps = int(steps)\n",
    "        lr_scheduler_fn = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps, 0)\n",
    "    elif lr_scheduler == 'fix':\n",
    "        lr_scheduler_fn = None\n",
    "    else:\n",
    "        raise Exception(\"lr schedule not implement\")\n",
    "        \n",
    "    return lr_scheduler_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda7990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:36.520355Z",
     "iopub.status.busy": "2022-06-29T15:28:36.519591Z",
     "iopub.status.idle": "2022-06-29T15:28:36.521907Z",
     "shell.execute_reply": "2022-06-29T15:28:36.521495Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.325959Z"
    },
    "papermill": {
     "duration": 0.068058,
     "end_time": "2022-06-29T15:28:36.522020",
     "exception": false,
     "start_time": "2022-06-29T15:28:36.453962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def distillation_method(distill_method,temp,feat_s,feat_t,module_list,trainable_list):\n",
    "    if distill_method == 'kd':\n",
    "        criterion_kd = DistillKL(temp)\n",
    "    elif distill_method == 'hint':\n",
    "        criterion_kd = HintLoss()\n",
    "        regress_s = ConvReg(feat_s[opt.hint_layer].shape, feat_t[opt.hint_layer].shape)\n",
    "        module_list.append(regress_s)\n",
    "        trainable_list.append(regress_s)\n",
    "    elif distill_method == 'semckd':\n",
    "        s_n = [f.shape[1] for f in feat_s[1:-1]]\n",
    "        t_n = [f.shape[1] for f in feat_t[1:-1]]\n",
    "       \n",
    "        criterion_kd = SemCKDLoss()\n",
    "        self_attention = SelfA(len(feat_s)-2, len(feat_t)-2, batch_size, s_n, t_n)\n",
    "        module_list.append(self_attention)\n",
    "        trainable_list.append(self_attention)\n",
    "        \n",
    "    return criterion_kd,module_list,trainable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0087b381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:36.639625Z",
     "iopub.status.busy": "2022-06-29T15:28:36.637916Z",
     "iopub.status.idle": "2022-06-29T15:28:36.642031Z",
     "shell.execute_reply": "2022-06-29T15:28:36.641544Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.342677Z"
    },
    "papermill": {
     "duration": 0.063519,
     "end_time": "2022-06-29T15:28:36.642154",
     "exception": false,
     "start_time": "2022-06-29T15:28:36.578635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the monitoring accuracy\n",
    "def accuracy_measuring_fn(monitor_acc):\n",
    "    \n",
    "    if monitor_acc == 'acc':\n",
    "        cal_acc = None\n",
    "    elif monitor_acc == 'AUC':\n",
    "        cal_acc = RocAucEvaluation\n",
    "    elif monitor_acc == 'ecgAcc':\n",
    "        cal_acc = cal_Acc\n",
    "    else:\n",
    "        raise Exception(\"monitor_acc is not implement\")\n",
    "        \n",
    "    return cal_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30866675",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:36.763340Z",
     "iopub.status.busy": "2022-06-29T15:28:36.762568Z",
     "iopub.status.idle": "2022-06-29T15:28:36.765025Z",
     "shell.execute_reply": "2022-06-29T15:28:36.764613Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.35597Z"
    },
    "papermill": {
     "duration": 0.066359,
     "end_time": "2022-06-29T15:28:36.765152",
     "exception": false,
     "start_time": "2022-06-29T15:28:36.698793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(confusion_matrix, axes, class_label,class_count, class_names, fontsize=14):\n",
    "\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    axes.set_ylabel('True label')\n",
    "    axes.set_xlabel('Predicted label')\n",
    "    axes.set_title(\"Confusion Matrix for the class - {} [{}]\".format(class_label,class_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822d84f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:36.881405Z",
     "iopub.status.busy": "2022-06-29T15:28:36.880768Z",
     "iopub.status.idle": "2022-06-29T15:28:36.887933Z",
     "shell.execute_reply": "2022-06-29T15:28:36.887535Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.369406Z"
    },
    "papermill": {
     "duration": 0.067025,
     "end_time": "2022-06-29T15:28:36.888098",
     "exception": false,
     "start_time": "2022-06-29T15:28:36.821073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_graphs(history,epochs):\n",
    "\n",
    "    epoch_list = [i for i in range(epochs)]\n",
    "    logs = list(history.keys())\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    rows = 2\n",
    "    columns = 3\n",
    "    grid = plt.GridSpec(rows, columns, wspace = .25, hspace = .50)\n",
    "    for i in range(len(logs)):\n",
    "        exec (f\"plt.subplot(grid{[i]})\")\n",
    "        if i <=5:\n",
    "            plt.plot(epoch_list, history[logs[i]])\n",
    "            plt.title(logs[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b40201d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:37.010252Z",
     "iopub.status.busy": "2022-06-29T15:28:37.009407Z",
     "iopub.status.idle": "2022-06-29T15:28:37.011101Z",
     "shell.execute_reply": "2022-06-29T15:28:37.011557Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.38363Z"
    },
    "papermill": {
     "duration": 0.065455,
     "end_time": "2022-06-29T15:28:37.011684",
     "exception": false,
     "start_time": "2022-06-29T15:28:36.946229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions(labels,predictions,threshold):\n",
    "    \n",
    "    # Generate the outputs using the threshold\n",
    "    labels_numpy = labels.cpu().detach().numpy().astype(np.int)\n",
    "    \n",
    "    y_label = np.zeros(labels_numpy.shape) #creating a dummy array\n",
    "\n",
    "    _, y_pre_label = torch.max(predictions, 1)\n",
    "    y_pre_label = y_pre_label.cpu().detach().numpy()\n",
    "\n",
    "    y_label[np.arange(labels_numpy.shape[0]), y_pre_label] = 1\n",
    "    y_prob = predictions.cpu().detach().numpy()\n",
    "    y_pre = predictions.cpu().detach().numpy() >= threshold\n",
    "\n",
    "    y_predictions = y_pre + y_label\n",
    "    y_predictions[y_predictions > 1.1] = 1\n",
    "    \n",
    "    return y_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6024d44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:37.135352Z",
     "iopub.status.busy": "2022-06-29T15:28:37.134504Z",
     "iopub.status.idle": "2022-06-29T15:28:37.136991Z",
     "shell.execute_reply": "2022-06-29T15:28:37.136598Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.397871Z"
    },
    "papermill": {
     "duration": 0.069489,
     "end_time": "2022-06-29T15:28:37.137100",
     "exception": false,
     "start_time": "2022-06-29T15:28:37.067611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visulaise_performance(ground_truth,predictions,history,epochs,threshold,class_names,classwise_sample_count):\n",
    "    \n",
    "    \n",
    "    challenge_metric = cal_Acc(ground_truth, predictions, threshold, num_classes=len(class_names))\n",
    "    print('Final Challenge Metric Score',challenge_metric)\n",
    "    \n",
    "    plot_graphs(history,epochs)\n",
    "    auroc,auprc = compute_auc(ground_truth, predictions)\n",
    "    \n",
    "    fig = go.Figure(data=[go.Table(header=dict(values=['classes','AUROC', 'AUPRC']),\n",
    "                 cells=dict(values=[class_names, auroc,auprc]))\n",
    "                     ])\n",
    "    fig.show()\n",
    "\n",
    "    \n",
    "    preds = get_predictions(ground_truth,predictions,threshold)\n",
    "    \n",
    "    macro_f1 = skm.f1_score(ground_truth.cpu().detach().numpy(), preds, average='macro')\n",
    "    micro_f1 = skm.f1_score(ground_truth.cpu().detach().numpy(), preds, average='micro')\n",
    "    weighted_f1 = skm.f1_score(ground_truth.cpu().detach().numpy(), preds, average='weighted')\n",
    "\n",
    "    print('macro_f1 : ', macro_f1)\n",
    "    print('micro_f1 : ', micro_f1)\n",
    "    print('weighted_f1 : ', weighted_f1)\n",
    "    \n",
    "    report = skm.classification_report(ground_truth.cpu().detach().numpy(),preds, target_names = class_names)\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    cm = skm.multilabel_confusion_matrix(ground_truth.cpu().detach().numpy(), preds)\n",
    "    \n",
    "    fig, ax = plt.subplots(6, 4, figsize=(22, 10))\n",
    "    \n",
    "    for axes, cfs_matrix, label, class_count in zip(ax.flatten(), cm, class_names, classwise_sample_count):\n",
    "        print_confusion_matrix(cfs_matrix, axes, label,class_count, [\"N\", \"Y\"])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30887287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:37.257864Z",
     "iopub.status.busy": "2022-06-29T15:28:37.257118Z",
     "iopub.status.idle": "2022-06-29T15:28:37.259764Z",
     "shell.execute_reply": "2022-06-29T15:28:37.259260Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.415518Z"
    },
    "papermill": {
     "duration": 0.067012,
     "end_time": "2022-06-29T15:28:37.259887",
     "exception": false,
     "start_time": "2022-06-29T15:28:37.192875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class SemCKDLoss(nn.Module):\n",
    "    \"\"\"Cross-Layer Distillation with Semantic Calibration, AAAI2021\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SemCKDLoss, self).__init__()\n",
    "        self.crit = nn.MSELoss(reduction='none')\n",
    "        \n",
    "    def forward(self, s_value, f_target, weight):\n",
    "        bsz, num_stu, num_tea = weight.shape\n",
    "        ind_loss = torch.zeros(bsz, num_stu, num_tea).cuda()\n",
    "\n",
    "        for i in range(num_stu):\n",
    "            for j in range(num_tea):\n",
    "                ind_loss[:, i, j] = self.crit(s_value[i][j], f_target[i][j]).reshape(bsz,-1).mean(-1)\n",
    "\n",
    "        loss = (weight * ind_loss).sum()/(1.0*bsz*num_stu)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ec7a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:37.378770Z",
     "iopub.status.busy": "2022-06-29T15:28:37.377934Z",
     "iopub.status.idle": "2022-06-29T15:28:37.379709Z",
     "shell.execute_reply": "2022-06-29T15:28:37.380172Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.429628Z"
    },
    "papermill": {
     "duration": 0.064542,
     "end_time": "2022-06-29T15:28:37.380310",
     "exception": false,
     "start_time": "2022-06-29T15:28:37.315768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DistillKL(nn.Module):\n",
    "    \"\"\"Distilling the Knowledge in a Neural Network\"\"\"\n",
    "    def __init__(self, T):\n",
    "        super(DistillKL, self).__init__()\n",
    "        self.T = T\n",
    "\n",
    "    def forward(self, y_s, y_t):\n",
    "        p_s = F.log_softmax(y_s/self.T, dim=1)\n",
    "        p_t = F.softmax(y_t/self.T, dim=1)\n",
    "        loss = nn.KLDivLoss(reduction='batchmean')(p_s, p_t) * (self.T**2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37029abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:37.511389Z",
     "iopub.status.busy": "2022-06-29T15:28:37.510442Z",
     "iopub.status.idle": "2022-06-29T15:28:37.512399Z",
     "shell.execute_reply": "2022-06-29T15:28:37.512770Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.442249Z"
    },
    "papermill": {
     "duration": 0.076208,
     "end_time": "2022-06-29T15:28:37.512910",
     "exception": false,
     "start_time": "2022-06-29T15:28:37.436702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch,dataloader,module_list,criterion_list,optimizer, leads,selected_leads,KD_args,scaler):\n",
    "    \n",
    "    # set modules as train()\n",
    "    for module in module_list:\n",
    "        module.train()\n",
    "        \n",
    "    # set teacher as eval()\n",
    "    module_list[-1].eval()\n",
    "    \n",
    "    \n",
    "    criterion_bce = criterion_list[0]\n",
    "    criterion_div = criterion_list[1]\n",
    "    criterion_kd = criterion_list[2]\n",
    "    \n",
    "    \n",
    "    student_model= module_list[0]\n",
    "    teacher_model = module_list[-1]\n",
    "    \n",
    "    \n",
    "    # Define the temp variable\n",
    "    epoch_start = time.time()\n",
    "    epoch_loss = 0.0\n",
    "    distillation_loss = 0.0\n",
    "    \n",
    "\n",
    "    for batch_idx, (inputs, labels, age_gender,input_indices) in (enumerate(dataloader)):\n",
    "        \n",
    "        #print('batch_idx:',batch_idx)\n",
    "        if selected_leads != None:\n",
    "            lead_pos = [leads[i] for i in selected_leads]\n",
    "            student_inputs = inputs[:,lead_pos,:]\n",
    "        else:\n",
    "            student_inputs = inputs\n",
    "            \n",
    "        teacher_inputs = inputs.to(device)\n",
    "        student_inputs = student_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        age_gender = age_gender.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_features,teacher_logits = teacher_model(teacher_inputs,age_gender, is_feat = True, preact= True)\n",
    "            teacher_features = [f.detach() for f in teacher_features]\n",
    "\n",
    "        \n",
    "        # Do the learning process, in val, we do not care about the gradient for relaxing\n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            student_features,student_logits = student_model(student_inputs, age_gender, is_feat = True,preact= True) #check outputs\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            logits_prob = sigmoid(student_logits)\n",
    "            \n",
    "\n",
    "            #saving the predictions and label\n",
    "            if batch_idx == 0:\n",
    "                labels_all = labels\n",
    "                predictions_all = logits_prob\n",
    "            else:\n",
    "                labels_all = torch.cat((labels_all, labels), 0)\n",
    "                predictions_all = torch.cat((predictions_all, logits_prob), 0)\n",
    "\n",
    "           \n",
    "            #BCE + KL_Loss\n",
    "            loss_bce = criterion_bce(student_logits,labels)     \n",
    "            loss_div = criterion_div( student_logits,teacher_logits) \n",
    "            #print('loss_bce:',loss_bce,'loss_div:',loss_div)\n",
    "            \n",
    "            if KD_args['distillation_type'] == 'semckd':\n",
    "                s_value, f_target, weight = module_list[1](student_features[1:-1], teacher_features[1:-1]) #calling self-attention\n",
    "                loss_kd = criterion_kd(s_value, f_target, weight) \n",
    "                \n",
    "            loss = KD_args['gamma'] * loss_bce + KD_args['alpha'] * loss_div + KD_args['beta'] * loss_kd\n",
    "\n",
    "                \n",
    "            loss_temp = loss.item() * inputs.size(0)\n",
    "            epoch_loss += loss_temp\n",
    "            \n",
    "            loss_kd = loss_kd.item() * inputs.size(0)\n",
    "            distillation_loss += loss_kd\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "  \n",
    "            \n",
    "    train_auprc = average_precision_score(y_true=labels_all.cpu().detach().numpy(), y_score=predictions_all.cpu().detach().numpy())\n",
    "    \n",
    "    # Print the train and val information via each epoch\n",
    "    epoch_loss = epoch_loss / len(dataloader)\n",
    "    distillation_loss = distillation_loss / len(dataloader)\n",
    "    \n",
    "   \n",
    "    print('Training: Epoch: {} train-Loss: {:.4f} distillation-Loss: {:.4f} train_auprc {} Cost {:.1f} sec'.format(epoch, epoch_loss,distillation_loss,train_auprc, time.time() - epoch_start))\n",
    "    return epoch_loss,train_auprc, distillation_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1a822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:37.641029Z",
     "iopub.status.busy": "2022-06-29T15:28:37.640074Z",
     "iopub.status.idle": "2022-06-29T15:28:37.643098Z",
     "shell.execute_reply": "2022-06-29T15:28:37.642535Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.468188Z"
    },
    "papermill": {
     "duration": 0.074034,
     "end_time": "2022-06-29T15:28:37.643224",
     "exception": false,
     "start_time": "2022-06-29T15:28:37.569190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def validation(valid_dataloader, model, criterion,  leads, selected_leads, batch_size=batch_size):\n",
    "    \n",
    "        \n",
    "        model.eval()\n",
    "        epoch_loss = 0\n",
    "        inp_channel = 12 if selected_leads == None else len(selected_leads)\n",
    "        dummy_input = torch.randn(batch_size,inp_channel, 4096, dtype=torch.float).to(device)\n",
    "        age_dummy_input = torch.randn(batch_size, 5, dtype=torch.float).to(device)\n",
    "        total_time  = 0\n",
    "        \n",
    "        #GPU-WARM-UP: will automatically initialize the GPU and prevent it from going into power-saving mode when we measure time\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy_input,age_dummy_input)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "            for batch_idx,(inputs ,labels,age_gender, _) in enumerate(valid_dataloader):\n",
    "                \n",
    "                \n",
    "                if selected_leads != None:\n",
    "                    lead_pos = [leads[i] for i in selected_leads]\n",
    "                    inputs = inputs[:,lead_pos,:]\n",
    "                else:\n",
    "                    inputs = inputs\n",
    "\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                age_gender = age_gender.to(device)\n",
    "                \n",
    "                starter.record()\n",
    "                logits = model(inputs,age_gender)\n",
    "                sigmoid = nn.Sigmoid()\n",
    "                logits_prob = sigmoid(logits)\n",
    "                ender.record()\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "                curr_time = starter.elapsed_time(ender)/1000 #Returns time elapsed in milliseconds\n",
    "                total_time += curr_time\n",
    "                \n",
    "                if 'Focal' in str(criterion):\n",
    "                    loss = criterion(logits,labels,valid_dataloader.dataset.classwise_sample_count,len(valid_dataloader.dataset))\n",
    "                else:\n",
    "                    loss = criterion(logits,labels)\n",
    "                loss_temp = loss.item() * inputs.size(0)\n",
    "                epoch_loss += loss_temp\n",
    "                \n",
    "\n",
    "                \n",
    "                #storing all the predictions\n",
    "                if batch_idx == 0:  \n",
    "                    labels_all = labels\n",
    "                    predictions_all = logits_prob\n",
    "                else:\n",
    "                    labels_all = torch.cat((labels_all,labels), 0)\n",
    "                    predictions_all = torch.cat((predictions_all, logits_prob), 0)\n",
    "                \n",
    "                    \n",
    "                \n",
    "                    \n",
    "        epoch_loss = epoch_loss / len(valid_dataloader)\n",
    "        \n",
    "        #This formula gives the number of examples our network can process in one second.\n",
    "        Throughput = (batch_size)/total_time\n",
    "        \n",
    "\n",
    "        \n",
    "        valid_auprc = average_precision_score(y_true=labels_all.cpu().detach().numpy(), y_score=predictions_all.cpu().detach().numpy())\n",
    "\n",
    "        print(\"Validation: loss:\",epoch_loss,\" valid_auprc:\",valid_auprc,\" total_time: \",total_time,\" secs\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        return labels_all,predictions_all,epoch_loss,valid_auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bfdc19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:37.769643Z",
     "iopub.status.busy": "2022-06-29T15:28:37.768749Z",
     "iopub.status.idle": "2022-06-29T15:28:37.770562Z",
     "shell.execute_reply": "2022-06-29T15:28:37.771056Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.491536Z"
    },
    "papermill": {
     "duration": 0.071637,
     "end_time": "2022-06-29T15:28:37.771194",
     "exception": false,
     "start_time": "2022-06-29T15:28:37.699557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def runner(idx,leads,selected_leads, student_model,module_list,train_dataloader,valid_dataloader,start_epoch,max_epoch,lr_scheduler,lr,criterion_list,optimizer,KD_args,scaler,save_dir,cal_acc):\n",
    "    \"\"\"\n",
    "    Training process\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_model_name = \"dummy\"\n",
    "    step_start = time.time()\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    history = {'distillation_loss':[],'train_loss': [], 'train_auprc':[],'val_loss':[],'val_auprc':[]}\n",
    "    \n",
    "\n",
    "    \n",
    "     \n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "\n",
    "        \n",
    "        #Update the learning rate\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "            logging.info('current lr: {}'.format(lr_scheduler.get_last_lr()))\n",
    "        else:\n",
    "            logging.info('current lr: {}'.format(lr))\n",
    "        \n",
    "        \n",
    "        #------------------------------------------\n",
    "        #                  TRAIN\n",
    "        #-------------------------------------------\n",
    "        \n",
    "        #Stores the predictions from last epoch\n",
    "                                              \n",
    "        train_loss,train_auprc,distillation_loss = train(epoch,train_dataloader,module_list,criterion_list,optimizer,leads,selected_leads,KD_args,scaler)\n",
    "        \n",
    "        labels,predictions,val_loss,val_auprc = validation(valid_dataloader,student_model,criterion_list[0], leads,selected_leads)\n",
    "        \n",
    "        history['distillation_loss'].append(distillation_loss)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_auprc'].append(train_auprc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_auprc'].append(val_auprc)\n",
    "    \n",
    "        \n",
    "        if val_auprc>best_acc:\n",
    "            best_acc = val_auprc\n",
    "            best_predictions = predictions\n",
    "            best_labels = labels\n",
    "            best_model_name = os.path.join('./', '{}-{:.4f}-best_model-{}.pth'.format(epoch, best_acc,idx))\n",
    "            #best_attn_model_name = os.path.join('./', '{}-{:.4f}-best_attn-{}.pth'.format(epoch, best_acc,idx))\n",
    "            print(\"------------Saving the model,Best auprc at epoch \",epoch,\" is:\",best_acc,\"----------------\")\n",
    "            torch.save(module_list[0], best_model_name)\n",
    "            #torch.save(trainable_list[-1],best_attn_model_name)\n",
    "            \n",
    "    print('Loading Thresholds...')\n",
    "    threshold = find_thresholds(best_labels.cpu().detach().numpy(), best_predictions.cpu().detach().numpy(), valid_dataloader.dataset.class_codes.tolist())\n",
    "    print('Thresholds generated...')\n",
    "    \n",
    "    print(\"==============================  model {} gives best acc {} for fold-{} ==============================  \".format(best_model_name,best_acc,idx))\n",
    "    visulaise_performance(best_labels,best_predictions,history,max_epoch,threshold,valid_dataloader.dataset.class_names,valid_dataloader.dataset.classwise_sample_count)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01202f50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:37.898997Z",
     "iopub.status.busy": "2022-06-29T15:28:37.892043Z",
     "iopub.status.idle": "2022-06-29T15:28:37.901987Z",
     "shell.execute_reply": "2022-06-29T15:28:37.901573Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.513301Z"
    },
    "papermill": {
     "duration": 0.075499,
     "end_time": "2022-06-29T15:28:37.902111",
     "exception": false,
     "start_time": "2022-06-29T15:28:37.826612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setting_params(idx,leads,selected_leads,teacher_model_path,opt,momentum,criterion_bce,weight_decay,lr_scheduler,lr,steps,gamma,KD_args,device):\n",
    "    \n",
    "    datasets = {}\n",
    "    datasets['train'], datasets['val'] = ECG(\"../input/physionet20205folds/\", str(idx),None,None).data_preprare()\n",
    "    train_dataloader = DataLoader(datasets['train'],batch_size=batch_size,shuffle = True,drop_last = True)\n",
    "    valid_dataloader = DataLoader(datasets['val'],batch_size=batch_size,shuffle = True)\n",
    "    \n",
    "    inp_leads = 12 if selected_leads == None else len(selected_leads)\n",
    "    \n",
    "    teacher_model = resnet18()\n",
    "    teacher_model.load_state_dict(torch.load(teacher_model_path, map_location=device)['model'])\n",
    "    teacher_model.to(device)\n",
    "    student_model = CustomResnet(input_channel=inp_leads).to(device)\n",
    "    #student_model = resnet18(in_lead = inp_leads).to(device)\n",
    "    \n",
    "    \n",
    "    for param in teacher_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if inp_leads == 12:\n",
    "        print(\"All 12 leads are considered\")\n",
    "    else:\n",
    "        print(\"The leads considered are:\",selected_leads)\n",
    "\n",
    "    \n",
    "    temp = KD_args['temp']\n",
    "    criterion_div =  DistillKL(temp)\n",
    "    \n",
    "    signal_data1 = torch.randn(batch_size,12,4096).to(device)\n",
    "    signal_data2 = torch.randn(batch_size,inp_leads,4096).to(device)\n",
    "    ag_data = torch.rand(batch_size,5).to(device)\n",
    "    \n",
    "    teacher_model.eval()\n",
    "    student_model.eval()\n",
    "    \n",
    "    feat_t, _ = teacher_model(signal_data1,ag_data, is_feat=True)\n",
    "    feat_s, _ = student_model(signal_data2,ag_data, is_feat=True)\n",
    "    print('Teacher features:',len(feat_t),\" Student features:\",len(feat_s))\n",
    "    \n",
    "    module_list = nn.ModuleList([])\n",
    "    module_list.append(student_model) #contains student model\n",
    "    trainable_list = nn.ModuleList([])\n",
    "    trainable_list.append(student_model)\n",
    "    \n",
    "    \n",
    "    criterion_kd,module_list,trainable_list = distillation_method(KD_args['distillation_type'],temp,feat_s,feat_t,module_list,trainable_list)\n",
    "    \n",
    "    criterion_list = nn.ModuleList([])\n",
    "    criterion_list.append(criterion_bce)    # classification loss\n",
    "    criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation, SemCKDLoss()\n",
    "    criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "    \n",
    "    module_list.append(teacher_model) #This contains student, Attn and teacher models\n",
    "    \n",
    "    criterion_list.cuda()\n",
    "    module_list.cuda()\n",
    "\n",
    "    \n",
    "    optimizer = get_optimizer(opt,trainable_list,lr,momentum,weight_decay)\n",
    "    lr_scheduler_fn = get_lr_scheduler(lr_scheduler,optimizer,steps,gamma)\n",
    "    #lr_scheduler_fn = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_dataloader), epochs=max_epoch)\n",
    "    cal_acc = accuracy_measuring_fn(monitor_acc)\n",
    "    #threshold = np.load(weight_list[idx])['arr_0']\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    logging.basicConfig(filename='./model'+str(i)+'.log', filemode='w', format='%(name)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    runner(idx,leads,selected_leads, student_model,module_list,train_dataloader,valid_dataloader,start_epoch,max_epoch, lr_scheduler_fn, lr,criterion_list,optimizer,KD_args,scaler,save_dir,cal_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cd7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:38.027305Z",
     "iopub.status.busy": "2022-06-29T15:28:38.026454Z",
     "iopub.status.idle": "2022-06-29T15:28:38.028319Z",
     "shell.execute_reply": "2022-06-29T15:28:38.028832Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.538962Z"
    },
    "papermill": {
     "duration": 0.06829,
     "end_time": "2022-06-29T15:28:38.028977",
     "exception": false,
     "start_time": "2022-06-29T15:28:37.960687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PARAMETERS\n",
    "\n",
    "max_epoch = 150\n",
    "no_folds = 1\n",
    "\n",
    "# Load the checkpoint\n",
    "start_epoch = 0\n",
    "\n",
    "weight_list = ['../input/physionet-2020/magic_weight0.npz', '../input/physionet-2020/magic_weight1.npz', '../input/physionet-2020/magic_weight2.npz','../input/physionet-2020/magic_weight3.npz', '../input/physionet-2020/magic_weight4.npz', '../input/physionet-2020/magic_weight_avg.npz']\n",
    "save_dir = \"./\"\n",
    "teacher_model_path = '../input/pretrained-teacher-models/SERes-0.6830-auprc-12Lead-state-dict.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44109bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:38.157569Z",
     "iopub.status.busy": "2022-06-29T15:28:38.156693Z",
     "iopub.status.idle": "2022-06-29T15:28:38.158900Z",
     "shell.execute_reply": "2022-06-29T15:28:38.159277Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.554631Z"
    },
    "papermill": {
     "duration": 0.073604,
     "end_time": "2022-06-29T15:28:38.159416",
     "exception": false,
     "start_time": "2022-06-29T15:28:38.085812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizers & Loss fn\n",
    "\n",
    "opt = \"adam\" #optimizer\n",
    "lr = 0.003  #the initial learning rate\n",
    "momentum = 0.9  #the momentum for sgd\n",
    "weight_decay = 1e-5 #the weight decay\n",
    "lr_scheduler = \"step\"        #['step', 'exp', 'stepLR', 'fix', 'cos'] the learning rate schedule\n",
    "gamma = 0.1 #learning rate scheduler parameter for step and exp\n",
    "steps = \"20,40,70\"  #the learning rate decay for step and stepLR\n",
    "monitor_acc = \"ecgAcc\"\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "KD_args ={'temp':4,'beta':40,'alpha':1,'gamma':1,'distillation_type':'semckd'}  # default = 4 in paper\n",
    "\n",
    "#criterion = WeightedFocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8777d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T15:28:38.277531Z",
     "iopub.status.busy": "2022-06-29T15:28:38.277003Z",
     "iopub.status.idle": "2022-06-30T02:01:06.746936Z",
     "shell.execute_reply": "2022-06-30T02:01:06.747430Z",
     "shell.execute_reply.started": "2022-05-30T09:37:50.565777Z"
    },
    "papermill": {
     "duration": 37948.531421,
     "end_time": "2022-06-30T02:01:06.747589",
     "exception": false,
     "start_time": "2022-06-29T15:28:38.216168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(no_folds):\n",
    "    print(\"------------------------ Fold-\"+str(i)+\" ------------------------\")\n",
    "    setting_params(3,leads,two_leads,teacher_model_path,opt,momentum,criterion,weight_decay,lr_scheduler,lr,steps,gamma,KD_args,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63965b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-30T02:01:07.269881Z",
     "iopub.status.busy": "2022-06-30T02:01:07.269056Z",
     "iopub.status.idle": "2022-06-30T02:01:07.270826Z",
     "shell.execute_reply": "2022-06-30T02:01:07.271286Z",
     "shell.execute_reply.started": "2022-05-30T09:48:01.551598Z"
    },
    "papermill": {
     "duration": 0.265269,
     "end_time": "2022-06-30T02:01:07.271421",
     "exception": false,
     "start_time": "2022-06-30T02:01:07.006152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = torch.load('../input/pretrained-teacher-models/SERes-0.6314-auprc-6Lead.pth')\n",
    "# state_dict = model.state_dict()\n",
    "#torch.save({'model':state_dict},'SERes-0.6314-auprc-6Lead-state-dict.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37989.489308,
   "end_time": "2022-06-30T02:01:09.967307",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-29T15:28:00.477999",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
